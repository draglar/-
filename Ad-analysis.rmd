---
title: "**Ad analysis**"
author: Ian
date: 08/01/2021
output: html_notebook
---


<h1>**Specifying the question**</h1>
<p> To identify individuals likely to click on ads
<h2>Metrics of success</h2>
<p>The analysis will be successful if I can find individuals likely to click on the ads

<h2>Context<h2/>
<p>A Kenyan entrepreneur has created an online cryptography course and would want to advertise it on her blog.<br>
She currently targets audiences originating from various countries. In the past,
she ran ads to advertise a related course on the same blog and collected data in the process.<br>
She has employed a Data Science Consultant to help her identify which individuals are most likely to click on her ads.

<h1>**Importing the dataset and libraries**</h1>

```{r imports}
library('data.table')
library('tidyverse')
library('chron') # For working with datetime

advertising <- fread('http://bit.ly/IPAdvertisingData')
```
Column descriptions
<p>'Daily Time Spent on Site': consumer time on site in minutes
<p>'Age': cutomer age in years
<p>'Area Income': Avg. Income of geographical area of consumer
<p>'Daily Internet Usage': Avg. minutes a day consumer is on the internet
<p>'Ad Topic Line': Headline of the advertisement
<p>'City': City of consumer
<p>'Male': Whether or not consumer was male
<p>'Country': Country of consumer
<p>'Timestamp': Time at which consumer clicked on Ad or closed window
<p>'Clicked on Ad': 0 or 1 indicated clicking on Ad</p>
<p>obtained from <a href='https://www.kaggle.com/fayomi/advertising/discussion/57499'>kaggle discussion</a><p>

since the timestamps show times on leaving and entering the site clicked on ad entry 0 implies they were leaving the site while 1 they were entering the site
<p>previewing the top of the dataset

```{r top_6}
head(advertising)
```
<h1>**Cleaning the dataset**<h1>

**checking the datatypes of the columns in the dataset**
```{r data_types}
str(advertising)
```
**Summary of the dataset**

```{r summary}
summary(advertising)
```
**Checking for null values**
```{r null}
colSums(is.na(advertising))
```
>There were no null values in the dataset

**Checking for Duplicates**

```{r}
dim(advertising[duplicated(advertising)])
```
>There were no duplicates found

**spliting the timestamo column to date and time**

```{r time_Stamp_split}
time_stamp <- advertising$Timestamp
parts <- t(as.data.frame(strsplit(time_stamp,' ')))

advertising$dates <- as.Date(parts[,1]) #saving dates
advertising$times <- as.times(parts[,2])#saving time
# view(advertising)
```
```{r col_assign}
age <- advertising$Age
area_income <- advertising$`Area Income`
time_on_site <- advertising$`Daily Time Spent on Site`
internet_usage <- advertising$`Daily Internet Usage`
gender <- as.character(advertising$Male)
ad <- as.character(advertising$`Clicked on Ad`)

date <- advertising$dates
time <- advertising$times
country <- advertising$Country
city <- advertising$City
ad_topic <- advertising$`Ad Topic Line`
```
<h2>**Checking for outliers**</h2>

```{r}
boxplot(time_on_site)$out
```
No outliers in amount of time spent on the site
```{r}
boxplot(age)$out
# outlier(advertising$Age)
```
>No outliers in the ages of the users
```{r}
boxplot(area_income)$out
# outlier(advertising$`Area Income`)
```
Some few outliers in the areas of income mostly the lower income areas, removing these may cause a loss of valuable information, hence will not be removed
```{r}
boxplot(internet_usage)$out
# outlier(advertising$Age)
```
there were no outliers in the internate usage time
```{r}
boxplot(time)
```
There were no outliers in the time users were accessing or leaving the site
```{r}
boxplot(date)
```
There were no outliers in the dates users were accessing or leaving the site

<h1><b>EDA</b></h1>
**Univariate Analysis**

```{r}
get.mode <- function(v){
  uniq <- unique(v)
  # gets all the unique values in the column
  # match (v, uniq) matches a value to the unique values and returns the index
  # tabulate (match (v, uniq)) takes the values in uniq and counts the number of times each integer occurs in it.
  # which.max() gets the index of the first maximum in the tabulated list
  # then prints out the uniq value
  uniq[ which.max (tabulate (match (v, uniq)))]
}
```
```{r}
mean(date); median(date); get.mode(date)
```
>Access to the site was balanced with a bit more before april
, moreover there being more activity in april than other months, specifically on fourth april
```{r}
max(date); min(date)
```
> dates when users were accessing or leaving the site ranged from january 1 ,2016 and august 24 ,2016

**The countries with the most consumers**

```{r}

table.country <- table(country) # creates a frequency table
view(table.country)#viewing the table
table.country <- table.country[order(-table.country)] # re-ordering the table
head(table.country,10) # previewing the ordered table
```
**The countries with the least consumers**
```{r}
tail(table.country)
```
**Countries where there were the most ad clicks**
```{r}
only.ad <- country[ad==1]
table.country.ad <- table(only.ad)
view(table.country.ad)
table.country.ad <- table.country.ad[order(-table.country.ad)]
head(table.country.ad,10)
```

**Cities with the most activity on the site**
```{r}

table.city <- table(city)
view(table.city)
table.city <- table.city[order(-table.city)]
head(table.city,10)
```
```{r}
only.ad <- city[ad==1]
table.city.ad <- table(only.ad)
view(table.city.ad)
table.city.ad <- table.city.ad[order(-table.city.ad)]
head(table.city.ad,10)
```
```{r}
mean(age); median(age); get.mode(age)
```
most consumers were 31 years, with the average age at 36 years implying its skewed to the left
```{r}
max(age); min(age)
```
ages ranged from minimum( 19 ) to maximum( 61 ) years
```{r}
quantile(age,probs=c(0.05,0.95))
```
most of the people ranged between 23 and 52 years
```{r}
var(age); sd(age)
```
There is little deviation(8.8) in ages when moving from one consumer to the next
```{r}
ggplot(advertising,aes(age))+ geom_density()
```
the ages are skewed to the left, alot of people are younger
```{r}
mean(time_on_site); median(time_on_site); get.mode(time_on_site)
```
average time on site was 65 minutes, with more people spending 62 minutes on site.
```{r}
max(time_on_site); min(time_on_site)
```
time on the site ranged from 32 to 91 minutes
```{r}
quantile(time_on_site,probs=c(0.05,0.95))
```
most people spent between 37.6 and 86 minutes in the site.
```{r}
var(time_on_site); sd(time_on_site)
```
There is some deviation(15.8) in time on site when moving from one consumer to the next, but given that it is totally dependent on preference it could be seen as a small deviation.
```{r}
ggplot(advertising,aes(time_on_site))+ geom_histogram(fill='#222222')
```
Time has two peaks around 40 and 80 minutes,showing two classes of consumers, ones who spend a longer time on site and those who spend less time.
```{r}
mean(time); median(time); get.mode(time)
```
The average time consumers accessed onr left the site was at noon, with most people accessing it at 5:39 pm
```{r}
max(time); min(time)
```
access times ranged all day (24 hours)
```{r}
quantile(time,probs=c(0.05,0.95))
```
most people accessed the site between 1:13 am and 22:49 pm

```{r}
mean(area_income); median(area_income); get.mode(area_income)
```
The average income area was at 55,000, while the income area with the most consumers was 61,833.9, since mean is lower than the median more consumers are above the midian
```{r}
max(area_income);min(area_income)
```
The areas of income ranged from 13996.5 to 79484.8
```{r}
quantile(area_income,probs=c(0.05,0.95))
```
most people were between 28275.30 and 73600.72 area income brackets
```{r}
var(area_income); sd(area_income)
```
There is a some deviation(13414) in area of income when moving from one consumer to the next, given that it is income .
```{r}
ggplot(advertising,aes(area_income))+ geom_density()
```
The density plot is skewed to the right implying alot more people were above the median price bracket
```{r}
ggplot(advertising,aes(area_income))+ geom_histogram(fill = "#222222", colour = "#038b8d")
```
```{r}
mean(internet_usage); median(internet_usage); get.mode(internet_usage)
```
the average internet usage was 180 minutes, however alot of consumers spent 167.22 minutes online
```{r}
max(internet_usage); min(internet_usage)
```
The range of the time spent was from 104.78 to 269.96 minutes
```{r}
quantile(internet_usage,probs=c(0.05,0.95))
```
most people spent between 113.5 t0 246.7 minutes on the internet
```{r}
var(internet_usage); sd(internet_usage)
```
There is little deviation(43.9) on internet usage when moving from one consumer to the next
```{r}
ggplot(advertising,aes(internet_usage))+ geom_density()
```
there were two peaks at around 125 and 225 minutes on the internet, showing two brackets of people spending different amounts of time on the internet
```{r}
ggplot(advertising,aes(internet_usage))+ geom_histogram(fill = "#222222", colour = "#038b8d")
```
```{r}
ggplot(advertising,aes(gender))+ geom_bar()
```
There were more females who accessed the site
```{r}
ggplot(advertising,aes(ad))+ geom_bar(fill='#222222')
```
There was an equal number of people whoa accessed the site from both not clicking ads and clicking them

<h2>Bivarriate</h2>

```{r}
# library('viridis')
ggplot(advertising,aes(gender,fill=ad))+ geom_bar()
```
Most females accessing the site had clicked an ad while most males visiting the site had not clicked an ad
```{r}
ggplot(advertising,aes(internet_usage,time_on_site))+ geom_point(alpha=0.5)+
  geom_quantile(size=1 ,alpha = 1,color="#1abc9c")
```
People who spent more time on the internet tended to stay longer on the site
```{r}
ggplot(advertising,aes(internet_usage,time_on_site,color=ad))+ geom_point(alpha=0.75)+
  geom_quantile(size=0.9 ,alpha = 1,quantiles=c(0.25,0.5,0.75))
```
Most people who clicked on an ad spent less time on the site and the internet compared to those who did not click an add.
However considering the groups individually consumers spent less time on the site the longer they spent on the internet
```{r}
ggplot(advertising,aes(age,internet_usage))+ geom_point(alpha=0.5)+
  geom_quantile(size=1 ,alpha = 1,color="#1abc9c")
```
There was a decline in internet usage as consumers got older.
```{r}
ggplot(advertising,aes(age,internet_usage,color=ad))+ geom_point(alpha=0.75)+
  geom_quantile(size=0.9 ,alpha = 1,quantiles=c(0.25,0.5,0.75))
```
Internet usage from those who exited the site is increasing with age
Internet usage for those who clicked was fairly constant with a slight decline with age, moreover most were 35 years and above(an older generation)
```{r}
ggplot(advertising,aes(age,time_on_site))+ geom_point(alpha=0.5)+
  geom_quantile(size=1 ,alpha = 1,color="#1abc9c")
```
Time on site went down the older the consumer got, Content may be geared towards a younger demographic.
```{r}
ggplot(advertising,aes(age,time_on_site,color=ad))+ geom_point(alpha=0.75)+
  geom_quantile(size=0.9 ,alpha = 1,quantiles=c(0.25,0.5,0.75))
```
Time on the site Consumers leaving the site increased with age, the content could be more relevant to consumers around 30 years or they are loyal to the site.
The time on the site was fairly constant with those who clicked the ad (around 52 minutes) at different ages.

```{r}
ggplot(advertising,aes(area_income, fill=ad,color='black'))+ geom_histogram()
```
most of people clicking on the ads ranged from 40000 and 60000 areas of income.
People leaving the site ranged from 50000 and above exceeded the amount of people coming into the site through ads
```{r}
ggplot(advertising,aes(age,area_income))+ geom_point(alpha=0.5)+
  geom_quantile(size=1 ,alpha = 1,color="#1abc9c")
```

Areas of income decreased as age increased
```{r}
ggplot(advertising,aes(area_income,age,color=ad))+ geom_point(alpha=0.75)+
  geom_quantile(size=0.9 ,alpha = 1,quantiles=c(0.25,0.5,0.75))
```
The areas of income of those leaving the site were increasing with age, while those who clicked on ads decreased slightly with age
average age of those clicking the ad was 40
```{r}
ggplot(advertising,aes(time_on_site,area_income))+ geom_point(alpha=0.5)+
  geom_quantile(size=1 ,alpha = 1,color="#1abc9c")
```
The time on site increased with The area of income.
```{r}
ggplot(advertising,aes(time_on_site,area_income,color=ad))+ geom_point(alpha=0.75)+
  geom_quantile(size=0.9 ,alpha = 1,quantiles=c(0.25,0.5,0.75))
```
areas of income was fairly constant for those clicking ads, area of income may not have much of an impact on time on site by those clicking ads, however
time on site increases as area of income decreases meaning it could have an impact on retention of consumers
```{r}
advert <- subset(advertising, select = c(1:4,10,12))
heatmap(cor(advert),Rowv = NA, Colv = NA,scale = "column", margins = c(10,10))
```
<ul>
From the heatmap I observed:
<li> There was a strong relationship on clicking an ad and the age of the consumer

<li>There is a positive relationship between daily internet usage and (time on site and area income).<br>
With an increase in internet usage times so did time<br>
As the Area income increased time on site increased.

<li>There was a negative relationship between age and (Time spent on site, Area income and daily internet usage),
implying more younger consumers spending more time on the site and the internet.<li>

<h1>**Challenging the solution**</h1>
> The task could probably have been performed better by a machine learning model complementing the analysis.

<h1>**Conclusion**</h1>

In conclusion those likely to click ads fit into the following descriptions:
<ul>
<li> Consumer  ages ranging between 34 and 46
<li> internet usage ranging between 130 and 160 minutes a day
<li> area on income ranging between 40000 and 60000
<li> Consumers from the following countries :
Australia,Ethiopia,Turkey,Liberia,Liechtenstein,South Africa,Afghanistan,France,Hungary,Mayotte
<li> Consumers from the followint cities:
Lake David,Lake James,Lisamouth,Michelleside,Millerbury,Robertfurt,South Lisa,West Amanda,West Shannon,Williamsport
</ul>
<h1>**Supervised Modeling**</h1>
<p> so the model needs to predict whether someone will click an add

```{r}
library('tidyverse')
view(advertising)
```
```{r}
library('caret')
library('mlbench')
label_encode <- function (column,dataframe){
  data <- dataframe[[column]]
  new_data <- as.integer(factor(data))
  # print(new_data)
  return (new_data)
}
```
```{r}
advertising$city_new <- label_encode('City',advertising[,-c(5,9)])
advertising$country_new <- label_encode('Country',advertising[,-c(5,9)])
# label_encode('City',advert)
```
```{r}
adverts <- advertising[,-c(5,6,8,9)]
head(adverts)
```
```{r}
library('rpart')
library(rattle)

adverts$Class <- adverts$`Clicked on Ad`
adverts <- adverts[,-c(6)]
```
```{r}
#data splicing
set.seed(6)
train <- sample(nrow(adverts),size = ceiling(0.80*nrow(adverts)),replace = FALSE)
# training set
train_set <- adverts[train,]
# Validation set
Validation_set <- adverts[-train,]
```
```{r}
tree <- rpart(Class~.,data=train_set,method = "class",
              control = rpart.control(minsplit = 8, cp = 0.1))
# Visualize the decision tree with rpart.plot
# rpart.plot(tree, nn=TRUE)
#Testing the model
pred <- predict(object=tree,Validation_set[,-c(10)],type="class")
t <- table(Validation_set$Class,pred)
return (t)

accur <- cm_2x2_accuracy(t)
print(accur)

confusionMatrix(t)
# xt$byClass
```
```{r}
# I know this is probably overkill but im probably going to use it other than here
get_train_validation <- function (dataset){
  set.seed(6)
  # Geting the row numbers for train sample (80% of the dataset)
  train <- sample(c(1:nrow(dataset)),size = ceiling(0.80*nrow(dataset)),replace = FALSE)
  # training set == part of the dataset in the train sample
  train_set <- dataset[train,]
  # Validation set == part of the dataset not in the train sample
  Validation_set <- adverts[-train,]
  # fix for R not accepting multiple argument returns
  sets <- list("Train" = train_set, "Validation" = Validation_set)
  return (sets)
}
rpart_tree <- function (train_set, Validation_set, min_split=8, Cp=0.1,min_depth =2, maxdepth = 30){
  tree <- rpart(Class~.,data=train_set,method = "class",
              control = rpart.control(minsplit = min_split, cp = Cp,
               mindepth = min_depth,maxdepth = maxdepth,xval = 10
              )
   )
  #Testing the model
  pred <- predict(object=tree,Validation_set[,-c(10)],type="class")
  t <- table(Validation_set$Class,pred)
  accur <- cm_2x2_accuracy(t)

  hyper <- list('minsplit' = min_split, 'cp' = Cp, 'mindepth' = min_depth,'maxdepth' = maxdepth,'xval' = 10)
  resul <- list("tree" = tree, "Accuracy" = accur, 'Hyper_parameters' = hyper)
  return (resul)
}
# Accuracy was Evaluated using a confusion matrix
# tp = True positive
# fp = False positive
# fn = False negative
# tp = True negative
cm_2x2_accuracy <- function (t){
  tp <- t[[1]]
  fp <- t[[2]]
  fn <- t[[3]]
  tn <- t[[4]]
  acuracy <-(tp+tn )/ (tp+fp+fn+tn)
  return (acuracy)
}
```
```{r}
accuracy <- 0
minsplit <- c(5,10,15, 20)
cp <- c(0.1,0.01,0.001,0.0001)
min_dpth <- c(1:7)

for (min_split in minsplit){
  for (Cp in cp){
    for (mn_dpth in min_dpth){
      # Geting different train and validation sets on each run
      set <- get_train_validation(adverts)
      results <- rpart_tree(set$Train,set$Validation,min_split=min_split , Cp=Cp, min_depth = mn_dpth )
      # Checking to see if the new accuracy is greater than the existing best accuracy
      if (results$Accuracy > accuracy){
        accuracy <- results$Accuracy
        best_tree <- results$tree
        best_hypers <- results$Hyper_parameters
      }
      # print(results$Accuracy)
    }
  }
}
# set <- get_train_validation(adverts)
print(list('Best Accuracy :',accuracy*100,'With Hyperparameters at:',best_hypers))
rpart.plot(best_tree, nn=TRUE)
```
Rpart with an accuracy of 96.5
```{r}
svm_class <- function (train_set,Validation_set,c=0.01){
  classified <- svm(Class~.,data=train_set,
                    type = 'C-classification',
                    kernel = 'linear',
                    C = c
  )
  pred <- predict(object=classified,Validation_set[,-c(10)],type="class")

  t <- table(Validation_set$Class,pred)
  accur <- cm_2x2_accuracy(t)

  resul <- list("model" = classified, "Accuracy" = accur, 'Hyper_parameter' = c)
  return (resul)
}

```
```{r}
accuracy <- 0
C_ <- c(0.001,0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2,5)
for (i in C_){
  # Geting different train and validation sets on each run
  set <- get_train_validation(adverts)
  results <- svm_class(set$Train,set$Validation, c = i)
  # Checking to see if the new accuracy is greater than the existing best accuracy
  if (results$Accuracy >= accuracy){
    accuracy <- results$Accuracy
    best_model <- results$model
    best_hypers <- results$Hyper_parameter
  }
}
print(list('Best Accuracy :',accuracy*100,'With Hyperparameters at:',best_hypers))

```
```{r}
library('kernlab')
grid <- expand.grid(C = c(0,0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2,5))
set.seed(3233)
train_set$Class <- as.factor(train_set$Class)
svm_Linear_Grid <- train(Class~.,data=train_set, method = "svmRadial",
                         type = 'C-classification',
                    trControl=trctrl,
                    preProcess = c("center", "scale"),
                    tuneGrid = grid,
                    tuneLength = 10
)

svm_Linear_Grid



```
